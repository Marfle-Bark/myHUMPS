%
% This file is an example file demonstrating how to use the sigcomm-alternate class for papers
% submitted in partial fulfillment for assignments in the Computer Networking and Distributed Systems
% classes, CSE 4344 and CSE 5344/7344, at Southern Methodist University. The sigcomm-alternate 
% LaTeX class is used for submissions to CCR. It is a goal of at least CSE 5344/7344 that the students
% will submit at least paper submitted for an assignment to CCR for publication; therefore, this example
% file conforms to the CCR submission guidelines.
%
% Author: Cameron A. Keith 
% Creation Date: 8 October 2014
% Version: 0.3
%

\documentclass{sigcomm-alternate}
\usepackage{cite}
\usepackage{textgreek}
\usepackage{fixltx2e}
%---- The front matter contains packages being used, definitions, etc. including the title and author
%---- information for this work.

%---- Title
% The title of this work. For course assignments, the title must include the course number. 
\title
{
WiBi MAC: Biased Medium Access Control for WiFi
}

%---- Authors
% The authors are placed all within a single LaTeX author box. This is a CCR guideline that is not true of
% all ACM publications.
\numberofauthors{3}

\author{
% Each author is identified by name, simple affiliation, and email address. The command \alignauthor (no
% curly braces required) must precede each author name. The command \affaddr (curly braces required) 
% must precede each line in the affiliation. The command \email{ } (curly braces required) must by used to 
% identify the email address for each author. When more than three authors exist, a tabular environment will
% need to be used to align the authors across multiple lines.
\alignauthor Cameron A. Keith\\
\affaddr{Computer Science and Engineering Department\\
 	Southern Methodist University\\
	Dallas, Texas USA}\\
\email{ckeith@smu.edu}
%
\alignauthor Anna A. Carroll\\
\affaddr{Computer Science and Engineering Department\\
 	Southern Methodist University\\
	Dallas, Texas USA}\\
\email{aacarroll@smu.edu}
%
\alignauthor Dylan C. Fansler\\
\affaddr{Computer Science and Engineering Department\\
 	Southern Methodist University\\
	Dallas, Texas USA}\\
\email{dfansler@smu.edu}
%
\and
\alignauthor Ethan Busbee\\
\affaddr{Computer Science and Engineering Department\\
 	Southern Methodist University\\
	Dallas, Texas USA}\\
\email{ebusbee@smu.edu}
%\and  % use '\and' if you need 'another row' of author names
%------ Uncomment the following for editorial submissions to CCR
%\begin{tabular}{c}
%{\normalsize This article is an editorial note submitted to CCR. It has NOT been peer reviewed.}\\
%{\normalsize The authors take full responsibility for this article's
%technical content. Comments can be posted through CCR Online.}
%\end{tabular}
%}
}

%--- Begin the Document
\begin{document}

%---- Create the title
\maketitle

%---- Abstract
% The abstract is a 100 word summary of the problem addressed in the paper, the basic thesis and/or 
% the contributions of the paper, and the primary results and conclusions presented in the paper.



% I rewrote the Abstract - Dr. Engels

\begin{abstract}
%Current Medium Access Control, MAC, Protocols are based on the premise that every device 
%has an equal probability to transmit to their data across a network. This method 
%attempts to reduce the collisions in the network by having all users who want to send 
%a packet wait a random time interval between 0 and a minimum starting back-off,
% and increasing a particular senders minimun back-off each time their packet does not
% sent. This paper attempts to discover if a Wireless MAC protocol based off of using the 
%current frame number as the upper bound of the random number function in an attempt to 
%see if a biased priority can improve the average complete time of stream transmittion time accross a 
%particuar network.
%
%This paper looks to improve the overall network efficiency by using a MAC protocol that biases towards Streams of shorter sizes by basing the initial back off of the packet transmission off the current frame number being sent instead of a system declared minimum initial value. This approach should improve the average time to transmit data from all nodes on the network.
In this paper, we present WiBi MAC, a biased Medium Access Control (MAC) protocol for WiFi. The standard medium access approach utilized by WiFi relies upon Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) combined with slotted Aloha. This approach achieves a fair packet access scheme that is similar to Processor Sharing (PS). Biased scheduling approaches, such as Shortest Remaining Processing Time (SRPT), have been shown to yield better throughput and faster expected completion times than PS in computer process scheduling. The WiBi MAC for Wifi approximates SRPT in order to achieve greater throughput and utilization of the wireless medium. WiBi MAC operates by having each node choose a maximum potential back off time that is proportional to the number of packets that are ready to send. By favoring packets belonging to short streams over packets belonging to longer streams, we achieve a simulated throughput improvement of XXXX over the standard fair exponential back off approach used by WiFi.
% The percent improvement needs to be explicitly stated. This is the main conclusion that you should have from this work
\end{abstract}

%----Main Body of the Paper

\section{Introduction}

% YOU DON"T NEED THE BRACKETS AROUND EACH SECTION

% COMMENTS - The Introduction should start with a statement of the problem. This should be followed by motivation of the problem, and why the reader cares about it. Next is a general overview of your approach followed by a summary of your main conclusions. The last paragraph has a one sentence summary of each section. I've written an example below.

%Connecting to the internet has never been easier than it is in today's world. From when computers used to take an entire room to hold them, limited connectivity to other computers outside of Universities or governmental building, and specialty training. We now can carry computers around with us, in bags, pockets, or even wear them, giving us the ability to seemlessly connect to countless others with readily available connections to Wireless networks. The ability to join a Wireless network is as simple as a few clicks of a mouse or taps on a phone, and suddenly the world is at your finger tips. The ability for people to connect from around the world is increasing at an ever expanding rate with wireless access being introduced to the most remote places. With this sudden surge in increased accessibility and user interaction comes the question of is the current connection the best we have or can we improve? 

%Current MAC protocols operate with the assumption that all data being sent should be equal regardless of the type of data or the sender \cite{5340799, FairScheduling}. Under this protocol, packets from a computer using a video service such as Netflix or youTube have equal priority as a computer sending out an email. While this works well when you are attempting to balance out all users so no user is above another, with a large number of users it can begin to slow the average time to complete sending a stream of data. Though not the best protocol available, it is also not the worst available, but it also has the possibility of improvement. As it stands now, most research in unfair MAC protocols have resolved around one of two types, malicious behavior, behaving with the intent to disrupt other users,\cite{DDoS} or selfish behavior, behaving to improve your own performance\cite{Kyasanur}

% WRITE IN THE PRESENT TENSE - THIS PAPER DOES NOT "ATTEMPT" IT DOES
%This paperwill discuss  cheating/biased MAC protocol by changing the random back-off delay before sending a packet from a number between 0 and a set minimum, to 0 and the current Frame number in current stream. This MAC protocol as a result has a natural bias towards nodes that are sending a smaller stream as they are able to keep the variable maximum for there back off delay low compared to a node that is sending a longer stream. The protocol should also be able to interact in an environment where it is the only node using this and have the network remain at the same effeciency level or have improved effciency as a result.



% THIS IS THE NEW INTRO
%
%
%Current WiFi MAC protocols are based on the network having a fair/equal chance for any node on thenetwork to send a packet they have ready to send. This approach is similar to a basic  Processor Sharing algorithm. This paper discusses a different approach by testing different methods of biasing WiFi MAC protocols by using an algorithm similar to Shortest Remaining Processing Time (SRPT)  to determine the maximum for a pckets Random back off. In addition we will be testing this apporach along with using the current frame number of the packet being sent as the maximum value for the back off, as well as a dynamic approach of varying the back off window based on the level of traffic in the network. 

%These approaches are looking to improve the through put of the network by looking to improve the mean time to complete the data streams from nodes in a network. This approach would prefer nodes that have shorter data streams being sent leadinging to a higher number of completed data streams over a period of time when compared to the current fair exponential back off approach used by WiFi. 

%First paragraph here. // start with the basic statement - WiFi MAC protocol is inefficient and should be improved - motivate why an improved performance (name this) is beneficial and needed

Current WiFi MAC protocol is primarily designed around achieving a completely fair protocol where all nodes in the network have an equal probability to interact with the router. This allows for the network to allocate resources fairly between the nodes at the expense of reducing the overall network throughput performance. The standard WiFi protocol works well in environments where all of the data traffic is fairly similar in size and priority, but begins to suffer performance degradation when you have users with starkly different data sizes. A WiFi protocol that instead of optimizing for fairness of access optimizes for near-maximum throughput on the network would cause no negative effects when implemented in a network that has very similar data traffic size, and in the case where a network has different sizes of data, would increase the throughput of the network by having users with smaller data sizes transmit all of their data faster than a user that is sending a larger amount of data. 

% Second paragraph here. // Discuss how the current MAC protocol is similar to processor sharing including the back off approach.

Current WiFi works in the following way: when a node has a data packet to send, it selects a random number between 1 and a standard minimum value which it uses to establish a back off time for the number of slots to wait to pass before sending its data. This is similar to processor sharing in which programs have a uniformly random chance to be selected for access to the processor. Processor sharing also has functionality similar to WiFi MAC's exponential back off, for when nodes have a collision, which prevent larger programs have an increased delay between times they have access to the processor to ensure that they do not monopolize the processor ensuring that the smaller programs have the ability to use the processor as well.
%this last sentence still needs to be fixed

%Third paragraph here // Discuss solution of SRPT and modification of the back off range and how these are biased solutions.

SRPT achieves the highest throughput, lowest mean completion time, when applied to a single channel processor\cite{schrage}. It achieves this by always choosing the next program that has the lowest remaining process time, assuming that the time needed to complete processing is known at the start. A similar process can be implemented in the WiFi MAC protocol with the assumption that a node knows the size of the packet stream it needs to send. The node would then base the upper bound of the back off random number selector on the size of the packet stream that the node has left to send. As a node sends data over the network, it gradually lowers its upper bound allowing it to have a higher chance of sending its remaining packets in a shorter time frame. This approach biases the network in favor of packet streams that were small in the beginning as well as streams that are near completion. In the case that the size of the packet stream is unknown, a different way to bias the network towards streams of a smaller size is to base the upper bound of the random back off on the current frame number of the packet in the stream being sent. This approach requires that all streams start with the same range for the back off and gradually increase the range as the node sends more and more packets successfully across the network with larger streams having a higher upper bound than the smaller streams. The final approach to creating a biased protocol that improves the throughput of the network is to dynamically alter the bounds of the back off based on the current level of traffic in the network as well as the size of the stream remaining. If the node detects that the network traffic has increased and that it has a long stream remaining, then the node will increase the delay between its own packets to allow other nodes to communicate.

%Fourth paragraph here // Give basic results and the main conclusions achieved.


To determine the theoritical throughput improvements achieved by our protocols we construct a model that is similar to the model proposed in \cite{Unfair, Harchol}. Based on this, we conclude that our results will be similar to what was achieved by papers \cite{Unfair, Harchol} and that the expected throughput improvement of using a Biased protocol based on of the premise of SRPT is between 25\% and 300\% and depending on the network load and the \textalpha \ used in the Bouned Pareto distribution, all streams should experience a decrease in their mean time to send when compared to the standard WiFi approach.

The remainder of this paper is organized as follows. In Section~\ref{section:background} we discuss past improvements to WiFi MAC protocols as well as some of the challenges a protocol encounters when operating in a wireless network. In Section~\ref{section:relatedwork} we review related work to medium access control. In Section~\ref{section:analysis} we discuss the results of the four different simulations: standard WiFi MAC, MAC protocol based on the SRPT algorithm, back off based on the current frame number in the stream, and lastly a dynamic back off window changed based on the traffic level in the network. Finally, we draw the relevant conclusions in Section~\ref{section:conclusions}.


%
%
%
%


\section{Background}
\label{section:background}

% NEED RELATED WORK ON THE WIFI MAC, NOT JUST MAC PROTOCOLS. ALSO, NEED RELATED WORK ON THE PERFORMANCE OF SRPT VS PS AND OTHER RELATED PROCESSOR SCHEDULING APPROACHES.

%THIS IS A NICE ESSAY, BUT MUCH OF IT BELONGS IN THE INTRODUCTION - THE TRUE RELATED WORK IS THOSE WORKS THAT TRIED TO MODIFY WIFI MAC FOR BETTER PERFORMANCE OR PROCESSOR SCHEDULING EVALUATIONS. FIND THESE TYPES OF RELATED WORK TO INCLUDE HERE.
This section is an explanation of the history and past improvements to the WiFi MAC protocol as well as some of the challenges of a wireless network. For wireless networks, a principle problem is the hidden terminal problem. In a wired network, it is possible to sense when another node is being sent, and a collision will occur \cite{869217}. In a wireless network, however, this cannot be done, resulting in the need for an alternative method of handling collisions \cite{6574961}. Thus, wireless MAC protocols like Slotted-Aloha were created.  In the Slotted Aloha Protocol, a node can only be sent at the beginning of a time slot \cite{5340799}. This ensures that one node can finish sending before another one is sent, reducing the number of collisions.  Under a network with a light load, this approach has a low chance of collision. However, collisions can still occur if two nodes are sent in the same time slot. Under heavy loads, the probability that a node will be sent in the same time slot as another node will increase. Most research into this problem is based on reducing collisions while keeping the distribution of sent nodes fair.  An improvement on the Slotted Aloha Protocol is the Frameless ALOHA protocol, which uses a random access scheme to decide which nodes should be sent in which spot \cite{6336861}. Another attempt at improving the Slotted-ALOHA protocol is the Generalized Slotted ALOHA protocol, in which nodes are sent according to their probability of being transmitted successfully. However, an issue with this protocol is that if every node is attempting to maximize its own transmission rate, then the network can jam. \cite{4548143 }.  The Multiple Access with Collision Avoidance for Wireless (MACAW) protocol is one attempt at improving on the Slotted-Aloha Protocol and solving the hidden terminal and unfair MAC problems. MACAW introduces fairness, in which every stream sent on the network is treated equally. This contrasts our proposal in that we create an optimal MAC protocol by prioritizing certain streams, thus creating an unfair MAC protocol that optimizes the network.

\section{Related Work}
\label{section:relatedwork}

% THIS PART BELONGS IN THE RELATED WORK SECTION
In research done by \cite{Lucani} focused on the objective of improving the throughput of Wireless LAN by attempting to find an alternative MAC protocol to the current protocol. Their researched focused on the creation of a FAIR  MAC algorithm \cite{Lucani} that gave all nodes in a network equal priority for accessing the network. They determined that their algorithm worked to increase the throughput of a mixed data network, with VoIP and normal data, versus the current MAC algorithm. They've chosen a different path to achieve an outcome similar to this paper which suggests that there can be other ways to improve the throughput of a WiFi MAC protocol.

Other research involving unfair WiFi MAC protocols has been conducted on how to detect and prevent unfair behavior from a node on the network. Some has focused on merely detecting and blocking the misbehaving node, while others have suggested implementing a punishment of sorts on the misbehaving nodes. Ways that nodes can misbehave in order to increase their own performance is to refuse to forward packets in order to save energy, or to select a smaller back-off in order to increase throughput for their own traffic. These misbehaving nodes can degrade the network throughput for other, well-behaved nodes. \cite{Kyasanur}. 	

Most research has been done on detecting the manipulation of the random back off\cite{Suresh}, detecting if a node is cheating by altering the delay period between SIFS and DIFS\cite{Domino}. Thise approaches tend to only benefits a single user as they crowd out other users as they maximize their bandwidth \cite{Domino}. As a result it also decreases the throughput of the network as other user's data is put on hold they think the communication channel is always busy. This research acknowledges the issues that occur in the network when a single user behaves unfairly and attempts to monitor a network and correct misbehaving nodes for the improvement of the network. Their research is beneficial to this paper by giving insight to current unfair methods as well as creating additional protocol requirements to be tested against in future work performed on this subject.

A similar task to creating an unfair MAC protocol that everyone follows is detecting when a single user, or a small group of users, is being unfair and the process of handling them. A few approaches to remediate unfair behavior have been to exclude the misbehaving node from routing operations, encourage nodes to cooperate by penalizing misbehavior, or to incentives good behavior by paying nodes for cooperating. Another protocol detects unfair behavior by having a sender transmit an RTS (Request to Send) after waiting for a randomly selected number of slots in the range [0; CW].After the initial transmission between hosts, the receiving host sends with their acknowledgement: a random value that the sender then uses as the back off counter for each subsequent transmition during the stream. \cite{Kyasanur} With this protocol, if a receiving node receives a packet before the appropriate number of frames has passed, then the sending host is not obeying the Protocol and can then be handled accordingly. Their protocol could possibly work in the protocols suggested in this paper by using the router as the controller for who can send data when effectively turning the network in to a single core processor. The downside to this approach is the large overhead associated with the router as the coordinator for determining when a user can send data which may negate the gains from using the protocols mention with in this paper.

For a generalized processor sharing approach to flow control the worst-case bounds on delays and backlogs are derived for the purpose of applying constraints to leaky bucket sessions in arbitrary topology networks of Generalized Processor Sharing (GPS) \cite{Parek} servers. In the Generalized Processor Sharing research, allocating network to users of integrated service networks is viewed with the context of rate-based flow control being used. A major assumption made by GPS, is that the design of the network was one that used virtual circuit connection-based packets. The combination of GPS alongside Leaky bucket admission control will allow this type of network to make a maximized range of worst-case performance calculations for packet throughput and delay; this is brought about by using a packet-by-packet service discipline called PGPS \cite{demers}. There is good flexibility across user usage since each user can get wildly different values based on their environment; however, its efficiency is based solely on the servers conserving work. 
With this user flexibility of service in mind, an analysis is then made of broader classes of the network. Only a subset of the sessions that are created are leaky bucket constrained, because of this each session is given succinct bounds that are independent of the behavior of any of the other sessions. The bounds are also completely independent of the network's overall topology. However, these bounds are only shown to hold for each session that is guaranteed a backlog. This backlog must also have a clearing rate that exceeds the token arrival rate of its leaky bucket, so that there aren't multiple tokens interfering with the process.

A much broader class of networks, called Consistent Relative Session Treatment (CRST) networks are analyzed for the case in which all of the sessions suffer from having leaky bucket constraints. The process first begins with an algorithm that has presented with characteristics that show the internal traffic in terms of their average rate and the burstiness of packets; it is shown that all CRST networks are stable. The next step in the process is a method that is presented to yield bounds on a session delay and backlog given this internal traffic characterization. The links of each route are treated collectively, which will yield tighter bounds than those that result from finding the summation of all the worst-case delays (backlogs) at each of the links in the route. The bounds on delay and backlog for each session are efficiently computed from a universal service curve, and it is shown that these bounds are achieved by using staggered greedy regimes when an independent session's relaxation holds;  the staggering comes from the propagation delay that is also incorporated into the model.
Processor scheduling implies that the code segments (from here on we will refer to them as tasks) needing to be performed are to be assigned to a particular processor for execution at a particular time. Because many tasks can be potential candidates for execution, it is necessary to represent the collection of tasks in a manner that will conveniently represent the relationships among the tasks. A directed graph or precedence graph representation is probably the most popular and most appropriate representation to map processor scheduling. The nodes in these graphs can represent by independent operations or parts of a single program that are related to each other in time. The directed paths between nodes imply that a partial ordering or precedence relation exists between the tasks. Associated with each node is a second number that refers to the time required by a hypothetical processor to execute the code that is represented by the node itself. If the processors are identical, then any task can essentially run on any processor provided that its precedence requirements are satisfied. 

The major problem associated with the study of processor scheduling is the amount of computation time needed to locate a suitable schedule. In order to cover definitions, let us just say that an efficient algorithm is one that requires an amount of time that is bounded in the length of its input by some polynomial and an inefficient algorithm is one which essentially requires an enumeration of all possible solutions before the best solution can be selected \cite{Gonzalez}. The algorithm whose running times are exponential in the number of tasks to be scheduled can characterize solutions of these types. For most of the problems of interest in processor scheduling, no efficient algorithm is known; in fact, interestingly enough it is actually known that if an efficient algorithm for these problems could be constructed, then an efficient algorithm could be constructed for a large family of seemingly intractable problems. That is, it is at least as difficult to compute as the hardest problem in the family of problems capable of being solved by nondeterministic algorithms in polynomial time. It includes such problems as whether or not a propositional formula can be satisfied, whether or not a graph possesses a clique of a given size, and a version of the well-known traveling salesman problem. 


This paper's purpose however, is to prove that implementing an unfair protocol where network traffic is sent according to a certain priority will actually improve throughput for all nodes in the network. The proposed protocol will ideally also maintain its increased performance even when all nodes on the network are not operating under the same protocol. 




\section{Analysis}
\label{section:analysis}

%A basic simulation was used to analyse and compare the results from the standard fair WiFi protocol versus the previously mentioned biased protocols. The protocols were tested with a network consisting of **** end nodes with the nodes sending data following the distribution of data according to\cite{Douceur}. The standard WiFi protocol is used as the basis of comparison that the other protocols are measured against as a protocol that does not doesn't at least match the throughput of WiFi's current protocol dos not meet the requirements of improving the throughput the network. In the simulation, it is assumed that all end nodes on the network are following the same protocol, everynode behaves in the same manner as other nodes.

In this paper we've decided on a theoretical approach to determining the improvement that a biased MAC protocol can give compared to a fair MAC protocol. The assumptions used in our model are, the data streams being sent follow a heavy-tailed distribution, specifically a Bounded Pareto Distribution with variables, k = 332, p = 10\textsuperscript{10}, \textalpha\  = 1.1
 that the network load does not exceed a value of 1, representative of the decimal percent of the network bandwidth in use, all nodes experience the same amount of delay in the network, the network does not have a central authority that dictates who can send, and finally that the network is free of collisions between packets. 

These assumptions allow us to look more directly to compare current WiFi protocols to biased WiFi protocols on through-put with out the interference of outside variables. These assumptions are similar to the ones that Harchol-Balter and Bansal use in their paper \cite{Unfair} in which they analyze the efficiency of SRPT vs fair Processor Sharing. In there results they determined that SRPT preformed better than fair Processor Sharing in all cases of file sizes,even larger files preferred SRPT to fair sharing \cite{Unfair}, for load values .96 and \textalpha\ = 1.1 and load value .999 and \textalpha\ = 1.5 all jobs perform better than standard Processor sharing, which makes SRPT an attractive possibility for a MAC Protocol. Additionally, in their research, Harchol-Balter and Bansal needed to consider what component of the webserver to apply SRPT to in order to complete their expirement. They had to decide which component had the highest load first in order to know which resource needed to be scheduled first: the CPU, the disk to memory bandwidth, or the bandwidth allocated to them by the ISP. In our research, however, we will only need to consider applying the SRPT scheduling to the frames being sent, due to the point-to-point attribute of the MAC SubLayer. 

Our first protocol is based purely on SRPT. It is assumed that the size of the stream is known, and therefore smaller streams can be prioritized over larger streams. The random back-off that determines when frames are sent is set depending on the number of remaining frames the stream has left to send. In the case that we cannot tell the stream sizes, a variation that bases the random back-off on the frame number being sent instead of the remaining stream size. Our third variation on the SRPT Protocol will be called Dynamic Backoff. In Dynamic Backoff, a node sends at a standard rate, periodically checking the network traffic level. If the traffic level rises to a certain level, the node will check its own rate and slow down its transmission rate based on its traffic levels. 


To implement SRPT, we must define priority levels similar to what was done in \cite{Unfair}. Each node will store its traffic in 16 priority queues. They are numbered 0 to 15, with 0 having the highest priority frames and 15 having the lowest priority frames. The priority queues are then emptied into the network in numerical order. As in, priority queue i will not send its traffic until priority queue i-1 and all the queues before it are empty. The way traffic is prioritized is according to the heavy-tailed workloads mentioned earlier. This means that traffic is divided in a way such that for all queues i\textsubscript{1}\ \textless \ i\textsubscript{2}\ \textless \  ... \textless \  i\textsubscript{16}, i\textsubscript{1} contains 50\% of streams are smaller than what is contained in i\textsubscript{2}. Inversely, i\textsubscript{15}\ needs to contain the largest .5\% to 1\% of streams to be sent by the node. 

SRPT improves the throughput of transmissions with smaller stream sizes substantially. This is a given, however, and it is more interesting to observe that the negative effect of SRPT on transmissions with larger stream sizes are negligible. This is due to the heavy-tailed property mentioned above. This entails that less than 1\% of the longest stream sizes take up over half the network load. The heavy tailed distribution is characterized by the following equation: \(PR{X > x }\sim x^{-\alpha}\) where 0 \textless \space \alpha  \space \textless \space 2. 


\section{Conclusions}
\label{section:conclusions}

 
%The distribution of files sizes that were sent over the simulated network was based on two assumptions: first being the files were representative of a commercial network and second that the distribution of file sizes has not changed dramatically since 1999 when the original distribution was calculated \cite{Douceur}. 

Using an SRPT method to achieve a biased MAC indeed tends to yield higher throughput in the network. Throughput 



\bibliographystyle{plain}
\bibliography{unfairMACPaperbib}  % unfairMACPaperbib.bib is the name of the Bibliography in this case

\balancecolumns
%---- End the Document
\end{document}




